
<!DOCTYPE html>
<html lang="it"><head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Home Assignment 3</title>
    <base href="../">
    <link rel="stylesheet" type="text/css" href="assets/css/standard.css">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css"
        integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"
        integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
        integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk"
        crossorigin="anonymous"></script>
    <script src="assets/js/math-render.js"></script>

    <script src="assets/js/dark-mode.js"></script>
</head>


<body><header>
    <nav id="navbar">
        <ul>
            <li><a href="">Home</a></li>
            <li><a href="preview/notes.html">Notes</a></li>
        </ul>
    </nav>
</header>


    <main>
        <h1 id="main-title"> Home Assignment 3 </h1>
        <h1>Preprocessing (33 points)</h1>
<h2>Importance of Preprocessing (6 points)</h2>
<p>We have the following data points:</p>
<h3>a)</h3>
<table><thead><tr><th>Person</th><th>Age in years</th><th>Income in thousands of USD</th><th>Paid off</th></tr></thead><tbody>
<tr><td>A</td><td>47</td><td>35</td><td>yes</td></tr>
<tr><td>B</td><td>22</td><td>40</td><td>no</td></tr>
<tr><td>C</td><td>21</td><td>36</td><td>-</td></tr>
</tbody></table>
<p>$$
d_A = \sum_{i=1}^2 (x_i - y_i)^2 = (21-47)^2 + (36-35)^2 = 677
$$</p>
<p>$$
d_B = (21-22)^2 + (36-40)^2 = 17
$$</p>
<p>Therefore, we get $d_A &gt; d_B$, and we can conclude the BoL should not give credit to C, according to the nearest neighbor algorithm.</p>
<h3>b)</h3>
<table><thead><tr><th>Person</th><th>Age in years</th><th>Income in thousands of USD</th><th>Paid off</th></tr></thead><tbody>
<tr><td>A</td><td>47</td><td>35000</td><td>yes</td></tr>
<tr><td>B</td><td>22</td><td>40000</td><td>no</td></tr>
<tr><td>C</td><td>21</td><td>36000</td><td>-</td></tr>
</tbody></table>
<p>$$
d_A = \sum_{i=1}^2 (x_i - y_i)^2 = (21-47)^2 + (36000-35000)^2 = 1000676
$$</p>
<p>$$
d_B = (21-22)^2 + (36000-40000)^2 = 16000001
$$</p>
<p>Therefore, we get $d_A &lt; d_B$, and we can conclude the BoL should give credit to C, according to the nearest neighbor algorithm.</p>
<h2>Input Centering (9 points)</h2>
<h3>a)</h3>
<p>Considering the following equations:</p>
<p>$$ z_n = x_n - \bar{x}, \forall n = 1, \dots, N $$</p>
<p>$$ \bar{x} = \frac{1}{N} X^T 1 $$</p>
<p>$$ \gamma = 1 - \frac{1}{N} 1 1^T $$</p>
<p>We can show that:</p>
<p>$$ Z = \gamma X $$</p>
<p>Indeed:</p>
<p>$$ z_n = x_n - \bar{x}, \forall n = 1, \dots, N $$</p>
<p>Turning this into a matrix form, we get:</p>
<p>$$ Z = X - 1 \bar{x}^T $$</p>
<p>$$ = X - 1 (\frac{1}{N} X^T 1)^T $$</p>
<p>Remembering that $(A B)^T = B^T A^T$, we get:</p>
<p>$$ Z = X -  \frac{1}{N} 1 1^T X $$</p>
<p>$$ = I X -  \frac{1}{N} 1 1^T X $$</p>
<p>$$ = (I -  \frac{1}{N} 1 1^T) X $$</p>
<p>$$ = \gamma X $$</p>
<p>Therefore, we have shown that $Z = \gamma X$.</p>
<h3>b)</h3>
<p>Considering that $Z$ is an $N \times D$ matrix and $\text{rank}(Z) = \text{rank}(Z^T)$. Using the rank-nullity theorem, we have:</p>
<p>$$ \text{rank}(A) + \text{rank}(\text{ker}(A)) = d $$</p>
<p>Citing a property of the rank:</p>
<p>$$ \text{rank}(A B) \leq \min(\text{rank}(A), \text{rank}(B)) $$</p>
<p>Therefore, we have that:</p>
<p>$$ \text{rank}(\gamma) + \text{rank}(\text{ker}(\gamma)) = N $$</p>
<p>And since $\text{rank}(\text{ker}(\gamma)) = 1$, we have $\text{rank}(\gamma) = N - 1$.</p>
<p>Thus, $\text{rank}(Z) = \text{rank}(\gamma X) \leq \min(\text{rank}(X), N - 1) &lt; N$.</p>
<h2>Input Whitening (18 points)</h2>
<h3>a)</h3>
<p>Given the following:</p>
<p>$$ \text{Var}(\hat{x}_1) = \text{Var}(\hat{x}_2) = 1 $$</p>
<p>$$ \mathbb{E}[\hat{x}_1] = \mathbb{E}[\hat{x}_2] = 0 $$</p>
<p>$$ x_1 = \hat{x}_1 $$</p>
<p>$$ x_2 = \sqrt{1 - \epsilon^2} \hat{x}_1 + \epsilon \hat{x}_2, \quad \text{for } \epsilon \in [-1, 1] $$</p>
<p>$$ \text{Cov}(\hat{x}_1, \hat{x}_2) = 0 $$</p>
<p>The last equation is given by the fact that the two variables are independent. Therefore, we already have the variance of $x_1$: $\text{Var}(x_1) = \text{Var}(\hat{x}_1) = 1$.</p>
<p>The variance of $x_2$ is given by:</p>
<p>$$ \text{Var}(x_2) = (\sqrt{1 - \epsilon^2})^2 \text{Var}(\hat{x}_1) + \epsilon^2 \text{Var}(\hat{x}_2) $$</p>
<p>$$ = 1 - \epsilon^2 + \epsilon^2 $$</p>
<p>$$ = 1 $$</p>
<p>Finally, the covariance between $x_1$ and $x_2$ is given by:</p>
<p>$$ \text{Cov}(x_1, x_2) = \sqrt{1 - \epsilon^2} \text{Cov}(\hat{x}_1, \hat{x}_1) + \epsilon \text{Cov}(\hat{x}_1, \hat{x}_2) $$</p>
<p>$$ = \sqrt{1 - \epsilon^2} \text{Var}(\hat{x}_1) $$</p>
<p>$$ = \sqrt{1 - \epsilon^2} $$</p>
<h3>b)</h3>
<p>Given the following:</p>
<p>$$ x = (x_1, x_2)^T $$</p>
<p>$$ \hat{x} = (\hat{x}_1, \hat{x}_2)^T $$</p>
<p>$$ f(\hat{x}) = \hat{w}_1 \hat{x}_1 + \hat{w}_2 \hat{x}_2 $$</p>
<p>This leads to the following equivalent statements:</p>
<p>$$ w_1 x_1 + w_2 x_2 = \hat{w}_1 \hat{x}_1 + \hat{w}_2 \hat{x}_2 $$</p>
<p>$$ w_1 \hat{x}_1 + w_2 (\sqrt{1 - \epsilon^2} \hat{x}_1 + \epsilon \hat{x}_2) = \hat{w}_1 \hat{x}_1 + \hat{w}_2 \hat{x}_2 $$</p>
<p>$$ w_1 \hat{x}_1 + w_2 \sqrt{1 - \epsilon^2} \hat{x}_1 + w_2 \epsilon \hat{x}_2 = \hat{w}_1 \hat{x}_1 + \hat{w}_2 \hat{x}_2 $$</p>
<p>This results in the following system of equations:</p>
<p>$$
\begin{cases}
(w_1 + w_2 \sqrt{1 - \epsilon^2}) \hat{x}_1 = \hat{w}_1 \hat{x}_1 \
w_2 \epsilon \hat{x}_2 = \hat{w}_2 \hat{x}_2
\end{cases}
$$</p>
<p>Thus, we arrive at the final conclusion that $f$ is linear in the correlated inputs:</p>
<p>$$
\begin{cases}
w_1 = \hat{w}_1 - \hat{w}_2 / \epsilon \sqrt{1 - \epsilon^2} \
w_2 = \hat{w}_2 / \epsilon
\end{cases}
$$</p>
<h3>c)</h3>
<p>Given the target function:</p>
<p>$$ f(\hat{x}) = \hat{x}_1 + \hat{x}_2 $$</p>
<p>With the constraint $C$:</p>
<p>$$ w_1^2 + w_2^2 \leq C $$</p>
<p>If we perform regression with the correlated inputs $x$, let's find the minimum value of $C$ such that the constraint is satisfied.</p>
<p>First, let's compute the values of $w_1$ and $w_2$:</p>
<p>$$ \hat{w}_1 = 1 $$</p>
<p>$$ \hat{w}_2 = 1 $$</p>
<p>Therefore:</p>
<p>$$ w_1 = 1 - 1 / \epsilon \sqrt{1 - \epsilon^2} $$</p>
<p>$$ w_2 = 1 / \epsilon $$</p>
<p>Now, let's compute the value of $C$:</p>
<p>$$ C = w_1^2 + w_2^2 $$</p>
<p>$$ = (1 - 1 / \epsilon \sqrt{1 - \epsilon^2})^2 + (1 / \epsilon)^2 $$</p>
<p>$$ = 1 + \frac{1 - \epsilon^2}{\epsilon^2} - \frac{2}{\epsilon} \sqrt{1 - \epsilon^2} + \frac{1}{\epsilon^2} $$</p>
<p>$$ = \frac{2}{\epsilon^2} - \frac{2 \sqrt{1 - \epsilon^2}}{\epsilon} $$</p>
<h3>d)</h3>
<p>Finally, let's compute the following limit:</p>
<p>$$ \lim_{\epsilon \to 0} C = \lim_{\epsilon \to 0} \left( \frac{2}{\epsilon^2} - \frac{2 \sqrt{1 - \epsilon^2}}{\epsilon} \right) = \infty $$</p>
<h1>Competition Design to Find Defective Products (24 points)</h1>
<h2></h2>
<p>Follows the theorem of generalization bound for selection from finite $\mathcal{H}$:</p>
<p>$$ \mathbb{P}(L(\hat{h}^<em>_S) \leq \hat{L}(\hat{h}^</em>_S, S) + \sqrt{\frac{\ln(M / \delta)}{2n}}) \geq 1 - \delta $$</p>
<p>Let's repeat our hypothesis:</p>
<p>$$ M = 20 $$</p>
<p>$$ \delta = 2 $$</p>
<p>We are looking for the minimum value of $n$ such that the following inequality is satisfied:</p>
<p>$$ \sqrt{\frac{\ln(M / \delta)}{2n}} \leq 0.04 $$</p>
<p>Therefore:</p>
<p>$$ \frac{\ln(20 / 2)}{2 \cdot 0.04^2} = 312.5 &lt; 313 = n $$</p>
<h2></h2>
<p>Given the following:</p>
<p>$$ n = 1800 $$</p>
<p>$$ \delta = 2 $$</p>
<p>We are looking for the maximum value of $M$ such that the following inequality is satisfied:</p>
<p>$$ \sqrt{\frac{\ln(M / \delta)}{2n}} \leq 0.04 $$</p>
<p>Therefore:</p>
<p>$$ 2 \exp(0.04^2 \cdot 2 \cdot 1800) \approx 634.7 &gt; 634 = M $$</p>
<h1>Combining Multiple Confidence Intervals (22 points)</h1>
<p>Given the following:</p>
<p>$$ i \in I = {1, 2, 3} $$</p>
<p>$$ S_i = S $$</p>
<p>$$ \text{CI}_i = [l_i, u_i] , \text{w.p.} , 1 - \delta_i $$</p>
<p>$$ 0.99 = \prod_I (1 - \delta_i) $$</p>
<p>$$ \delta = \delta_i = \delta_j , \text{for all} , i, j \in I $$</p>
<p>Let's compute the value of $\delta$:</p>
<p>$$ 1 - \sqrt[3]{0.99} \approx 0.0033 &lt; 0.004 = \delta $$</p>
<p>We could compute a more precise value for $\delta$, but I only need to show how to answer this question. Alex can choose any combination of the confidence interval endpoints such that:</p>
<p>$$ l_{\text{chosen}} \leq u_{\text{chosen}} $$</p>
<p>Because any such combination is a valid (at least 99%)-CI. Therefore, he should choose the combination that minimizes the length of the CI:</p>
<p>$$ \text{CI} = [\max(l_i), \min(u_i)] $$</p>
<h1>Early Stopping (21 points)</h1>
<h2>Neural Network with Early Stopping (21 points)</h2>
<p>Statistical bias, in the mathematical field of statistics, is a systematic tendency in which the methods used to gather data and generate statistics present an inaccurate, skewed, or biased depiction of reality. â€” [Wikipedia@bias]</p>
<h3>Predefined Stopping</h3>
<p>The $S_{\text{val}}$ has no influence on the choice of the target function $h_{t^*}$, so the bias is not present.</p>
<h3>Non-adaptive Stopping</h3>
<p>The target function $h_{t^<em>}$ that minimizes the validation error $\hat{L}(h_{t^</em>})$ is chosen. Therefore, the dataset is used to choose the best target function, which leads the final model to be biased by the validation set $S_{\text{val}}$.</p>
<h3>Adaptive Stopping</h3>
<p>$h_{t^*}$ is chosen from the sequence of hypotheses $h_1, h_2, h_3, \dots, h_t$. While the target function is not chosen based on the validation set, the sequence stops when the validation does not improve for a certain number of steps. Therefore, the sequence of models is biased by the validation set $S_{\text{val}}$. As a counterexample to the claim that the bias is not present, let's consider the case in which a different validation set $S'<em>{\text{val}}$ is used. Then, it would produce the sequence of hypothesis models $h_1, h_2, h_3, \dots, h_j$, and $j \neq t$. Let $j &gt; t$ and $h_j$ be the best model, thus the final model choice differs from the one obtained with the original validation set $S</em>{\text{val}}$. Therefore, we can conclude that the final model is biased by the validation set $S_{\text{val}}$.</p>
<h2></h2>
<p>I have already cited the theorem of generalization bound (see @hp-bound), so follows the solution for the two cases.</p>
<h3>Predefined Stopping</h3>
<p>We have $M = 1$, because we are only considering the final model:</p>
<p>$$ \mathbb{P}(L(\hat{h}^<em>_S) \leq \hat{L}(\hat{h}^</em>_S, S) + \sqrt{\frac{\ln(1 / \delta)}{2n}}) \geq 1 - \delta $$</p>
<h3>Non-adaptive Stopping</h3>
<p>We have $M = T$, where $T$ is the number of epochs and thus the number of models to consider:</p>
<p>$$ \mathbb{P}(L(\hat{h}^<em>_S) \leq \hat{L}(\hat{h}^</em>_S, S) + \sqrt{\frac{\ln(T / \delta)}{2n}}) \geq 1 - \delta $$</p>

    </main><footer>
    <p>&copy; 2024 danesinoo. All Rights Reserved.</p>
</footer>

</body>

</html>